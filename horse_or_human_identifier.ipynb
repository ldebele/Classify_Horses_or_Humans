{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd1e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip the dataset\n",
    "local_zip = './data/horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./data/horse-or-human')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ffcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory with our training horse pictures\n",
    "train_horse_dir = os.path.join('./data/horse-or-human/horses')\n",
    "\n",
    "# Directory with our training human pictures\n",
    "train_human_dir = os.path.join('./data/horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3118b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse01-0.png', 'horse01-1.png', 'horse01-2.png', 'horse01-3.png', 'horse01-4.png', 'horse01-5.png', 'horse01-6.png', 'horse01-7.png', 'horse01-8.png', 'horse01-9.png']\n",
      "['human01-00.png', 'human01-01.png', 'human01-02.png', 'human01-03.png', 'human01-04.png', 'human01-05.png', 'human01-06.png', 'human01-07.png', 'human01-08.png', 'human01-09.png']\n"
     ]
    }
   ],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(train_horse_names[:10])\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(train_human_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94c0556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training horse images 500\n",
      "total training human images 527\n"
     ]
    }
   ],
   "source": [
    "# you can also find out the total number of horse and human images in the directories.\n",
    "print('total training horse images', len(os.listdir(train_horse_dir)))\n",
    "print('total training human images', len(os.listdir(train_human_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08123073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at a few pictures.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for out graph;\n",
    "# we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# index for iteration over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4, nrows*4)\n",
    "\n",
    "pic_index += 8\n",
    "next_horse_pix = [os.path.join(train_horse_dir, fname)\n",
    "                 for fname in train_horse_names[pic_index-8:pic_index]]\n",
    "\n",
    "next_human_pix = [os.path.join(train_human_dir, fname)\n",
    "                 for fname in train_human_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_horse_pix + next_human_pix):\n",
    "    # Set up subplot;\n",
    "    # subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i+1)\n",
    "    sp.axis('Off')\n",
    "    \n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38fd8",
   "metadata": {},
   "source": [
    "# Building a small Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06642539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8e7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the imput shape is the desired size of the images 300x300 with 3 bytes color.\n",
    "    # The first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The thrid convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # only 1 output neuron. It will contain a value from 0-1 where,\n",
    "    # 0 for 1 class ('horse') and 1 for the other class('Human')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp review the network architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=RMSprop(learning_rate=0.001),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8dcf8e",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory('./data/horse-or-human/',\n",
    "                                                   target_size=(300, 300), # All the images will be resized to 300x300\n",
    "                                                   batch_size=128,\n",
    "                                                    # Since we use binary_crossentropy loss, we need binary labels\n",
    "                                                   class_mode ='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d4d49",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31195358",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=15,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5cc32",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "images = os.listdir(\"./data/tmp/images\")\n",
    "\n",
    "print(images)\n",
    "\n",
    "for i in images:\n",
    "    print()\n",
    "    # Predicting images\n",
    "    path = './data/tmp/images/' + i\n",
    "    img = image.load_img(path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x /= 255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    print(classes[0])\n",
    "    if classes[0] > 0.5:\n",
    "        print(i + \" is a human\")\n",
    "    else:\n",
    "        print(i + \" is a horse\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb2c8d",
   "metadata": {},
   "source": [
    "# Visualizing Intermediate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d21146",
   "metadata": {},
   "outputs": [],
   "source": [
    "importnput numpy as np\n",
    "import random\n",
    "from tf.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "\n",
    "# Define a new Model that will take an image as i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepare a random input image from the training set.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
    "\n",
    "# Scale by 1/255\n",
    "x /= 255\n",
    "\n",
    "# Run the image through the network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so you can have them as part of the plot\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Display the representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = feature_map.shape[1]\n",
    "    \n",
    "    # Tile the images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "      # Tile each filter into this big horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    \n",
    "    # Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa7da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
